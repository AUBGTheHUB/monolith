# Prerequisites:
# https://docs.docker.com/engine/swarm/key-concepts/
# https://docs.docker.com/engine/swarm/ingress/
# https://docs.docker.com/engine/swarm/networking/

# The docker stack deploy command uses the legacy Compose file version 3, which is located in the
# [V1 branch of the Compose repository](https://docs.docker.com/reference/compose-file/legacy-versions/).
# https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md
# https://docs.docker.com/engine/swarm/stack-deploy/
#
# You can check more which options in the old compose are not supported for docker stack deploy:
# https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#not-supported-for-docker-stack-deploy
# https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#depends_on
version: "3.8"

# https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#deploy
# https://medium.com/@kinghuang/docker-compose-anchors-aliases-extensions-a1e4105d70bd
x-deploy-default: &deploy-default
  # https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#restart_policy
  # The options which are not set use the default values
  restart_policy:
    condition: any
    max_attempts: 3
    window: 1m30s

  # https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#update_config
  # The options which are not set use the default values
  update_config:
    order: start-first
    # We set this to 1m30s as it ensures that at least one full health check cycle completes during the monitoring
    # period, which should be sufficient to detect most issues. The 30s gives us some buffer time.
    monitor: 1m30s
    # If the updated version's state is considered failure the service will be rollbacked automatically
    failure_action: rollback

  # https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#rollback_config
  # The options which are not set use the default values
  rollback_config:
    order: start-first
    # We set this to 1m30s as it ensures that at least one full health check cycle completes during the monitoring
    # period, which should be sufficient to detect most issues. The 30s gives us some buffer time.
    monitor: 1m30s
    # If the rollbacked version's state is considered failure, all tasks in the service will be paused to allow for
    # investigation
    failure_action: pause

services:
  web:
    image: ghcr.io/aubgthehub/monolith-web:${WEB_GIT_COMMIT_HASH:-latest}

    # For Caddy, we are going to rely on the docker ingress network and routing mesh to send traffic to our caddy
    # instances. We are doing so in order to use a "start-first" update config. If we used `mode: host, and bypassed
    # the routing mesh, when we redeploy caddy we would get "port already in use", as this update config will try to
    # spin up a new instance before removing the old one (rolling release)
    # https://docs.docker.com/engine/swarm/networking/#publish-ports-on-an-overlay-network
    # https://docs.docker.com/engine/swarm/ingress/
    # https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#long-syntax-1
    ports:
      - mode: ingress
        protocol: tcp
        published: 80
        target: 80
      - mode: ingress
        protocol: tcp
        published: 443
        target: 443

    # Using `deploy: *deploy-default` instead, would assign the entire anchored object to deploy without any possibility
    # of adding or overriding specific keys later.
    deploy:
      <<: *deploy-default

    # When running in Swarm mode, healthchecks are used to for maintaining the desired state of the service. If a
    # health check fails, the task (container) enters a FAILED state, and the Swarm orchestrator creates a new
    # replica task that spawns a new container in replacement of the unhealthy one.
    # https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#tasks-and-scheduling
    # CMD-SHELL: Uses the container's shell to run the command, allowing
    # use of shell features like expanding the $DOMAIN environment variable.
    # $$DOMAIN: The double dollar sign ensures that Docker Compose doesn't
    # try to substitute $DOMAIN itself. Instead, it sends the literal '$DOMAIN'
    # to the container, where the shell correctly expands it at runtime.
    # See https://docs.docker.com/reference/compose-file/interpolation/
    # We use HTTP localhost as we only want to check if the caddy service is up and running. If we make a request
    # towards https://thehub-aubg.com for example, we will go through Cloudflare, which could lead to misleading results
    # as we are not directly checking the liveness of Caddy itself.
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost/health"]
      interval: 1m
      timeout: 10s
      retries: 3
      # Caddy should be up and running under 10 seconds, but this is the maximum time it should realistically take.
      start_period: 30s

    volumes:
      # https://caddyserver.com/docs/running#docker-compose
      - caddy_data:/data
      - caddy_config:/config

  py-api:
    image: ghcr.io/aubgthehub/monolith-backend:${PY_API_GIT_COMMIT_HASH:-latest}

    # https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#deploy
    deploy:
      <<: *deploy-default

      # We are not publishing ports, as we don't want the py-api service directly exposed to the open internet
      # We are going to relly on the IPVS to distribute traffic between the replicas. When `py-api:8080` gets resolved
      # it points to virtual IP (VIP). The load balancing between the replicas happens via the IPVS at Layer 4.
      # See https://docs.docker.com/engine/swarm/networking/#configure-service-discovery
      endpoint_mode: vip

      # We are using two replicas of the API to increase reliability
      # https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#replicas
      # The options which are not set use the default values
      replicas: 2

    environment:
      # The values of the env variables are the paths where the decrypted secret is mounted into the container in an
      # in-memory filesystem.
      # IMPORTANT: Services should implement an app-level logic to read the decrypted values from those mounted files,
      # otherwise the values of these env variable will stay the mounted paths!
      # https://docs.docker.com/engine/swarm/secrets/#how-docker-manages-secrets
      # https://docs.docker.com/engine/swarm/secrets/#use-secrets-in-compose
      DATABASE_URL: /run/secrets/db-url
      SECRET_KEY: /run/secrets/secret-key
      SECRET_AUTH_TOKEN: /run/secrets/secret-auth-key
      RESEND_API_KEY: /run/secrets/resend-api-key

    # When running in Swarm mode, health checks are used for maintaining the desired state of the service. If a
    # health check fails, the task (container) enters a FAILED state, and the Swarm orchestrator creates a new
    # replica task that spawns a new container in replacement of the unhealthy one.
    # https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/#tasks-and-scheduling
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:$${PORT}/api/v3/ping"]
      interval: 1m
      timeout: 10s
      retries: 3
      # The container should start within 10 seconds, but this is the maximum time it should realistically take.
      start_period: 10s

    # https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#secrets
    secrets:
      - db-url
      - secret-key
      - secret-auth-key
      - resend-api-key

    volumes:
      # Storing the logs like this is temporary solution until more sophisticated monitoring and observability system
      # is implemented.
      - py_api_logs:/app/shared

volumes:
  # Create a named volume to store py-api logs
  py_api_logs:
  # As docker containers are ephemeral, if we do not persist the /data direcotry of Caddy, every time we do a deploy
  # or the container gets replaced with a new one, we would request new TLS certificates. and we could hit LetsEncrypt
  # rate limits for Certificate Issuance.
  # https://caddyserver.com/docs/conventions#data-directory
  # https://caddyserver.com/docs/running#docker-compose
  # https://letsencrypt.org/docs/rate-limits/#new-registrations-per-ip-address
  caddy_data:
  # Persist the /config directory if we ever need it in the future
  # https://caddyserver.com/docs/conventions#configuration-directory
  caddy_config:

# https://github.com/docker/compose/blob/v1/docs/Compose%20file%20reference%20(legacy)/version-3.md#secrets
secrets:
  # These docker secrets have already been set on the PROD and DEV VMs, that's why we use external: true.
  # To set a new secret key-value pair you may use: `prinf "toniMontanaSecret" | docker secret create toni -`
  db-url:
    external: true
  secret-key:
    external: true
  secret-auth-key:
    external: true
  resend-api-key:
    external: true
